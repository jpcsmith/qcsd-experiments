#!/usr/bin/env python3
"""Usage: fetch-script [options] [URLFILE] [OUTFILE]

Load each URL once with TCP and record the logs from the browser.

URLFILE is a text file with one URL per line.  If not provided or '-',
the URLs are read from stdin.  The results are written as JSON stream
to OUTFILE (defaults to stdout).

Options:
    --delay n
        Wait n seconds between each request for the same URL
        [default: 15.0].

    --max-attempts n
        Stop trying to collect a URL after n sequential failures
        [default: 3].

    --driver-path path
        Use Chromedriver binary at path [default: chromedriver].

    --ranks
        The data is a CSV of the form rank,domain

    --single-url url
        Fetch the specfied url instead of reading from URLFILE. When
        used, non-sucessful collections exit with an exit code of 1.

    --force-quic-on-all
        Force QUIC on all the domains, not just the domain of the
        main HTML page.
"""
import sys
import asyncio
import logging
from typing import Optional

import doceasy
from doceasy import Use
from lab import fetch_websites
from lab.fetch_websites import ProtocolSampler, ChromiumSessionFactory
from lab.sniffer import PacketSniffer

LOGGER = logging.getLogger("dep-fetch")


class _NoOpPacketSniffer(PacketSniffer):
    def pcap(self) -> bytes:
        return b""

    def start(self) -> None:
        pass

    def stop(self) -> None:
        pass


async def main(
    urlfile, outfile, delay: float, driver_path: str, max_attempts: int,
    ranks: bool, single_url: Optional[str], force_quic_on_all: bool,
):
    """Load each URL once with TCP and record the logs from the browser.
    """
    logging.basicConfig(
        format='[%(asctime)s] %(name)s - %(levelname)s - %(message)s',
        level=logging.INFO)
    LOGGER.info("Running dependency collection with arguments: %s", locals())

    if single_url:
        LOGGER.info("Using sole URL: %s", single_url)
        urls = [single_url]
    else:
        urls = (["https://" + domain for (_, domain) in urlfile] if ranks
                else [url for (url, ) in urlfile])
    if not urls:
        raise ValueError("No URLs provided to fetch.")

    sampler = ProtocolSampler(
        sniffer=_NoOpPacketSniffer(),
        session_factory=ChromiumSessionFactory(
            driver_path=driver_path,
            force_quic_on_all=force_quic_on_all
        ),
        delay=delay, max_attempts=max_attempts)

    async for result in sampler.sample_multiple((urls, {"h3-29": 1})):
        result["page_source"] = None
        result["packets"] = None

        outfile.write(fetch_websites.encode_result(result))
        outfile.write("\n")
        outfile.flush()
        if single_url and result["status"] != "success":
            sys.exit(1)

    LOGGER.info("Dependency collection complete.")


if __name__ == '__main__':
    asyncio.run(
        main(**doceasy.doceasy(__doc__, {
            "URLFILE": doceasy.CsvFile(mode='r', default='-'),
            "OUTFILE": doceasy.File(mode='w', default='-'),
            "--driver-path": str,
            "--delay": Use(float),
            "--max-attempts": Use(int),
            "--ranks": bool,
            "--single-url": doceasy.Or(str, None),
            "--force-quic-on-all": bool,
        }))
    )
