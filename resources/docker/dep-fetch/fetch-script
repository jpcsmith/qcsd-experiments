#!/usr/bin/env python3
"""Usage: fetch-script [options] [URLFILE] [OUTFILE]

Load each URL once with TCP and record the logs from the browser.

URLFILE is a text file with one URL per line.  If not provided or '-',
the URLs are read from stdin.  The results are written as JSON stream
to OUTFILE (defaults to stdout).

Options:
    --delay n
        Wait n seconds between each request for the same URL
        [default: 15.0].

    --max-attempts n
        Stop trying to collect a URL after n sequential failures
        [default: 3].

    --driver-path path
        Use Chromedriver binary at path [default: chromedriver].

    --ranks
        The data is a CSV of the form rank,domain

    --single-url url
        Fetch the specfied url instead of reading from URLFILE. When
        used, non-sucessful collections exit with an exit code of 1.

"""
import sys
import asyncio
import logging
from typing import Optional

import doceasy
from doceasy import Use
from lab import fetch_websites
from lab.fetch_websites import ProtocolSampler, ChromiumSessionFactory
from lab.sniffer import PacketSniffer

LOGGER = logging.getLogger("dep-fetch")


class _NoOpPacketSniffer(PacketSniffer):
    def pcap(self) -> bytes:
        return b""

    def start(self) -> None:
        pass

    def stop(self) -> None:
        pass


async def main(
    urlfile, outfile, delay: float, driver_path: str, max_attempts: int,
    ranks: bool, single_url: Optional[str],
):
    """Load each URL once with TCP and record the logs from the browser.
    """
    logging.basicConfig(
        format='[%(asctime)s] %(name)s - %(levelname)s - %(message)s',
        level=logging.INFO)
    LOGGER.info("Running dependency collection with arguments: %s", locals())

    if single_url:
        LOGGER.info("Using sole URL: %s", single_url)
        urls = [single_url]
    else:
        urls = (["https://" + domain for (_, domain) in urlfile] if ranks
                else [url for (url, ) in urlfile])
    if not urls:
        raise ValueError("No URLs provided to fetch.")

    sampler = ProtocolSampler(
        sniffer=_NoOpPacketSniffer(),
        session_factory=ChromiumSessionFactory(driver_path=driver_path),
        delay=delay, max_attempts=max_attempts)

    async for result in sampler.sample_multiple((urls, {"h3-29": 1})):
        result["page_source"] = None
        result["packets"] = None

        outfile.write(fetch_websites.encode_result(result))
        outfile.write("\n")
        outfile.flush()
        if single_url and result["status"] != "success":
            sys.exit(1)

    LOGGER.info("Dependency collection complete.")


if __name__ == '__main__':
    asyncio.run(
        main(**doceasy.doceasy(__doc__, {
            "URLFILE": doceasy.CsvFile(mode='r', default='-'),
            "OUTFILE": doceasy.File(mode='w', default='-'),
            "--driver-path": str,
            "--delay": Use(float),
            "--max-attempts": Use(int),
            "--ranks": bool,
            "--single-url": doceasy.Or(str, None),
        }))
    )
